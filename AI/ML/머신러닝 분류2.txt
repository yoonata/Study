머신러닝의 분류

머신러닝은 학습하려는 문제의 유형에 따라 크게 다음과 같은 세 가지로 분류할 수 있습니다.

1. 지도 학습(Supervised Learning)
2. 비지도 학습(Unsupervised Learning)
3. 강화 학습(Reinforcement Learning)

지도 학습(Supervised Learning)

지도 학습(Supervised Learning)이란 간단히 말해 선생님이 문제를 내고 그 다음 바로 정답까지 같이 알려주는 방식의 학습 방법입니다.
즉, 여러 문제와 답을 같이 학습함으로써 미지의 문제에 대한 올바른 답을 예측하고자 하는 방법입니다.
따라서 지도 학습을 위한 데이터에는 문제와 함께 그 정답까지 함께 알고 있는 데이터가 선택됩니다.
예를 들어, “장미꽃이 찍혀 있는 이미지 데이터”에 레이블로 “해당 장미꽃의 품종을 나타내는 텍스트“를 함께 입력하여 학습기를 지도 학습시키면, 다른 장미꽃이 찍힌 새로운 이미지를 받았을 때 해당 장미꽃의 품종이 무엇인지를 예측할 수 있게 되는 것입니다.
학습기(learner)에 데이터와 함께 입력하는 정답을 레이블(label)이라고 부릅니다.

지도 학습 모델

머신러닝에서 지도 학습을 위한 모델은 크게 분류(classification) 모델과 예측(prediction) 모델로 구분됩니다.
분류 모델은 사용하는 알고리즘에 따라 또다시 KNN(K Nearest Neighbor), 서포트 벡터 머신(Support Vector Machine, SVM), 의사결정 트리(decision trees) 등의 모델로 구분되며, 예측 모델로는 회귀(regression) 모델이 대표적으로 사용되고 있습니다.
분류 모델과 예측 모델 모두 지도 학습 모델이므로, 데이터와 레이블을 함께 학습시킨다는 공통점을 가집니다.
하지만 분류 모델은 학습 데이터의 레이블 중 하나가 결괏값이 되고, 예측 모델은 학습 데이터에서 도출된 함수식에서 계산된 임의의 값이 결괏값이 되는 점이 서로 다릅니다.

분류(classification) 모델

분류 모델은 레이블이 달린 학습 데이터로 학습한 후에 새로 입력된 데이터가 학습했던 어느 그룹에 속하는 지를 찾아내는 방법입니다.
따라서 분류 모델의 결괏값은 언제나 학습했던 데이터의 레이블 중 하나가 됩니다.
즉, 이미지를 통해 학습한 결과 새로운 이미지에 해당하는 숫자가 0인지 1인지를 파악하는 것입니다.
예를 들어, ‘가’, ‘나’, ‘다’라는 레이블이 달린 데이터를 분류 모델로 지도 학습한 후, 새로운 데이터를 분석한 결과는 반드시 ‘가’, ‘나’, ‘다’ 중의 하나가 되는 것입니다.
이러한 유형의 문제는 일상에서 흔히 접할 수 있는 문제이며, 따라서 이에 관한 연구가 많이 진행되어 있습니다. 또한 기업에서도 많은 관심을 가지고 있는 문제 중 하나입니다.

예측 모델(predictive model)

예측 모델도 분류 모델과 마찬가지로 지도 학습 모델이므로 레이블이 달린 학습 데이터로 학습하게 됩니다.
하지만 예측 모델은 분류 모델과는 달리 레이블이 달린 학습 데이터를 가지고 특징(feature)과 레이블(label) 사이의 상관관계를 함수식으로 표현하게 됩니다.
따라서 ‘가’, ‘나’, ‘다’라는 레이블이 달린 데이터를 예측 모델로 지도 학습하였다고 하더라도 분류 모델처럼 결괏값이 반드시 ‘가’, ‘나’, ‘다’ 중 하나가 되는 것이 아니라 해당 범위 내의 어떠한 값도 나올 수 있는 것입니다. 
이처럼 어떠한 값이 결과로 나올지 예상할 수 없으므로, 이를 예측 모델이라고 부릅니다.
이러한 예측 모델은 주가나 환율 분석 등과 같이 연속적인 범위 내의 값에서 그 결괏값을 예측하는 문제에 일반적으로 많이 활용됩니다.

비지도 학습(Unsupervised Learning)

선생님이 문제와 함께 정답(레이블)까지 알려주는 지도 학습과는 달리 비지도 학습(Unsupervised Learning)은 문제는 알려주되 정답까지는 알려주지 않는 학습 방식입니다. 
즉, 여러 문제를 학습함으로써 해당 데이터의 패턴, 특성 및 구조를 스스로 파악하여, 이를 통해 새로운 데이터에서 일정한 규칙성을 찾는 방법입니다.
비지도 학습은 구체적인 결과에 대한 사전 지식은 없지만 해당 결과 데이터를 통해 유의미한 지식을 얻고자 할 때 사용되며, 사람도 제대로 알 수 없는 본질적인 문제나 데이터에 숨겨진 특징이나 구조 등을 연구할 때 많이 활용됩니다.
머신러닝에서 비지도 학습을 위한 모델로는 군집화(clustering)가 대표적입니다.

군집화(clustering)

비지도 학습에 사용되는 데이터에는 레이블(label)이 명시되어 있지 않기 때문에 지도 학습과는 또 다른 방식으로 학습을 수행해야 합니다.
이때 가장 많이 사용되는 방법이 바로 입력된 데이터가 어떤 형태로 서로 그룹을 형성하는지를 파악하는 것입니다.
비지도 학습에서 가장 대표적으로 사용되는 이 모델은 군집화(clustering) 또는 클러스터링이라고 불립니다.
군집화는 레이블이 없는 학습 데이터들의 특징(feature)을 분석하여 서로 동일하거나 유사한 특징을 가진 데이터끼리 그룹화 함으로써 레이블이 없는 학습 데이터를 군집(cluster, 그룹)으로 분류합니다. 
그리고 새로운 데이터가 입력되면 지도 학습의 분류 모델처럼 학습한 군집을 가지고 해당 데이터가 어느 군집에 속하는지를 분석하는 것입니다.

군집(cluster)의 타당성 평가

비지도 학습에 사용되는 데이터에는 레이블(label)이 없으므로, 지도 학습처럼 단순정확도(accuracy)를 지표로 그 정확도를 평가할 수는 없습니다.
즉, 다음 그림과 같이 레이블이 없는 데이터 집합 내에서 최적의 군집 모양과 개수를 파악하기란 굉장히 어렵습니다.
군집을 만든 결과가 얼마만큼 타당한지는 군집간의 거리, 군집의 지름, 군집의 분산도 등을 종합적으로 고려하여 평가할 수 있습니다.
따라서 일반적으로 군집 간 분산(inter-cluster variance)이 최대가 되고 군집 내 분산(inner-cluster variance)이 최소가 될 때 최적의 군집 모양과 개수라고 판단하고 있습니다.

군집화의 분류

군집화의 주목적은 레이블이 없는 데이터 집합의 요약된 정보를 추출하여, 이를 가지고 전체 데이터 집합이 가지고 있는 특징을 찾는 것입니다.
이러한 군집화는 사용하는 알고리즘에 따라 크게 두 가지 기법으로 나눌 수 있습니다.

1. 분할 기법(partitioning methods)의 군집화
2. 계층적 기법(hierarchical methods)의 군집화

분할 기법의 군집화는 각 그룹은 적어도 하나의 데이터를 가지고 있어야 하며 각 데이터는 정확히 하나의 그룹에 속해야 한다는 규칙을 가지고 데이터 집합을 작은 그룹으로 분할하는 방식입니다. 
이러한 분할 기법의 군집화에는 k-means, k-medoids, DBSCAN 등의 기법 등이 있습니다.
계층적 기법의 군집화는 데이터 집합을 계층적으로 분해하는 방식으로 그 방식에 따라 또다시 집괴적(agglomerative) 군집화와 분할적(divisive) 군집화로 나눠집니다.

군집화의 활용

군집화는 매우 다양한 분야에서 사용되고 있는 머신러닝 방법으로, 의학 분야에서는 특정 질병에 대한 공간 군집 분석을 통해 질병의 분포 면적과 확산 경로 등을 파악하는 역학 조사 등에서 활용되고 있으며, 홍보 분야에서는 고객을 세분화할 때 군집화를 활용하고 있습니다.
또한, 통계 분야에서도 분석하고자 하는 데이터에 다양한 군집화 알고리즘과 방법론을 사용하여 데이터 분석에 활용해 나가고 있는 추세입니다.

강화 학습(Reinforcement Learning)

지도 학습과 비지도 학습이 학습 데이터가 주어진 상태에서 환경에 변화가 없는 정적인 환경에서 학습을 진행했다면, 강화 학습은 어떤 환경 안에서 정의된 주체(agent)가 현재의 상태(state)를 관찰하여 선택할 수 있는 행동(action)들 중에서 가장 최대의 보상(reward)을 가져다주는지 행동이 무엇인지를 학습하는 것입니다.
강화 학습은 주체(agent)가 환경으로부터 보상을 받음으로써 학습하기 때문에 지도 학습과 유사해 보이지만, 사람으로부터 학습을 받는 것이 아니라 변화되는 환경으로부터 보상을 받아 학습한다는 점에서 차이를 보입니다.
이러한 강화 학습은 사람이 지식을 습득하는 방식 중 하나인 시행착오를 겪으며 학습하는 것과 매우 흡사하여 인공지능을 가장 잘 대표하는 모델로 알려져 있습니다.

강화 학습의 동작 순서

강화 학습은 일반적으로 다음과 같은 순서대로 학습을 진행하게 됩니다.

1. 정의된 주체(agent)가 주어진 환경(environment)의 현재 상태(state)를 관찰(observation)하여, 이를 기반으로 행동(action)을 취합니다.
2. 이때 환경의 상태가 변화하면서 정의된 주체는 보상(reward)을 받게 됩니다.
3. 이 보상을 기반으로 정의된 주체는 더 많은 보상을 얻을 수 있는 방향(best action)으로 행동을 학습하게 됩니다.

강화 학습에서의 ‘관찰–행동–보상’에 이르는 일련의 과정을 경험(experience)이라고 부를 수 있습니다.
경험을 통해 학습하는 강화 학습에서 최단 시간에 주어진 환경의 모든 상태를 관찰하고, 이를 기반으로 보상을 최대화할 수 있는 행동을 수행하기 위해서는 이용(exploitation)과 탐험(exploration) 사이의 균형을 적절히 맞춰야 합니다.
이용(exploitation)이란 현재까지의 경험 중 현 상태에서 가장 최대의 보상을 얻을 수 있는 행동을 수행하는 것을 의미하고, 이러한 다양한 경험을 쌓기 위해서는 새로운 시도가 필요한데 이러한 새로운 시도를 탐험(exploration)이라고 합니다.
탐험을 통해 얻게 되는 경험이 언제나 최상의 결과일 수는 없기에 이 부분에서 낭비가 발생하게 됩니다. 즉, 풍부한 경험이 있어야만 더 좋은 선택을 할 수 있게 되지만, 경험을 풍부하게 만들기 위해서는 새로운 시도를 해야 하고 이러한 새로운 시도는 언제나 위험 부담을 가지게 됩니다.
예를 들어, 빵집에 가서 지금까지 자신이 먹어본 빵 중 가장 맛있는 빵을 고르는 것이 이용(exploitation)이 되며, 한 번도 먹어보지 못한 다른 빵을 고르는 것이 탐험(exploration)이 됩니다. 
만약 새로 고른 빵이 가장 맛있다고 느껴지면 다음 번 선택에서 이용될 수 있으나, 만약 맛이 없었다면 한 번의 기회를 낭비하게 된 것입니다.
따라서 이용과 탐험 사이의 적절한 균형을 맞추는 것이 강화 학습의 핵심이 되는 것입니다.

마르코프 결정 프로세스(Markov Decision Process, MDP)

강화 학습에서 보상을 최대화할 수 있는 방향으로 행동을 취할 수 있도록 이용과 탐험 사이의 적절한 균형을 맞추는데 사용되는 의사결정 프로세스가 바로 마르코프 결정 프로세스(Markov Decision Process, MDP)입니다.
MDP에서 행위의 주체(agent)는 어떤 상태(state)를 만나면 행동(action)을 취하게 되며, 각 상태에 맞게 취할 수 있는 행동을 연결해 주는 함수를 정책(policy)이라고 합니다. 
따라서 MDP는 행동을 중심으로 가치 평가가 이루어지며, MDP의 가장 큰 목적은 가장 좋은 의사결정 정책(policy) 즉 행동에 따른 가치(value)의 합이 가장 큰 의사결정 정책을 찾아내는 것입니다.
이러한 MDP는 여러 방식을 통해 풀 수 있으며, 일반적으로 동적 계획법(dynamic programming)인 가치 반복법(Value Iteration, VI)이나 정책 반복법(Policy Iteration, PI), 선형 계획법(linear programming)인 Q러닝(Q-Learning) 등을 사용하여 그 해를 구하게 됩니다.

강화 학습의 활용

강화 학습은 프로세스 제어, 네트워크 관리, 로봇공학 등 현재 다양한 분야에서 활용되고 있습니다.
우리에게 익숙한 인공지능인 알파고도 바둑의 기본 규칙과 자체 경기를 통해 습득한 3,000만 개의 기보를 학습한 후 스스로 대국하며 훈련하는 강화 학습 알고리즘을 사용하여 개발되었습니다.
또한, 자율 주행 자동차와 드론 분야 등에서도 강화 학습을 활용한 다양한 연구 및 시도가 활발히 진행되고 있습니다.