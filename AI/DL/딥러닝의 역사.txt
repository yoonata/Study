딥러닝의 역사

MIT가 2013년을 빛낼 10대 혁신기술 중 하나로 선정하고 가트너(Gartner, Inc.)가 2014 세계 IT 시장 10대 주요 예측에 포함시키는 등 최근들어 딥 러닝에 대한 관심이 높아지고 있지만 사실 딥 러닝 구조는 인공신경망(ANN, artificial neural networks)에 기반하여 설계된 개념으로 역사를 따지자면 최소 1980년 Kunihiko Fukushima에 의해 소개 된 Neocognitron까지 거슬러 올라가야 한다.
1989년에 얀 르쿤과 그의 동료들은 오류역전파 알고리즘(backpropagation algorithm)에 기반하여 우편물에 손으로 쓰여진 우편번호를 인식하는 deep neural networks를 소개했다. 알고리즘이 성공적으로 동작했음에도 불구하고, 신경망 학습에 소요되는 시간(10 개의 숫자를 인식하기 위해 학습하는 시간)이 거의 3일이 걸렸고 이것은 다른분야에 일반적으로 적용되기에는 비현실적인 것으로 여겨졌다.
많은 요소들이 느린 속도에 원인을 제공했는데, 그 중 하나는 1991년 Jürgen Schmidhuber의 제자였던 Sepp Hochreiter에 의해 분석된 vanishing gradient problem(지역최솟값에 머무르게 되는 원인)이었다. 또한 불연속 시뮬레이션에서 초기 상태를 어떻게 선택하느냐에 따라 수렴이 안되고 진동 또는 발산하는 문제, 트레이닝셋에 너무 가깝게 맞추어 학습되는 과적합 (Overfitting) 문제, 원론적으로 생물학적 신경망과는 다르다는 이슈들이 끊임 없이 제기되면서 인공신경망은 관심에서 멀어졌고 90년대와 2000년대에는 서포트 벡터 머신 같은 기법들이 각광받게 된다.
본격적으로 딥 러닝이란 용어를 사용한 것은 2000년대 딥 러닝의 중흥기를 이끌어간다고 평가할 수 있는 제프리 힌튼과 Ruslan Salakhutdinov에 의해서이며, 기존 신경망의 과적합 문제를 해결하기 위해 이들은 unsupervised RBM(restricted Boltzmann machine)을 통해 학습시킬 앞먹임 신경망(Feedforward Neural Network)의 각 층을 효과적으로 사전훈련(pre-trainning)하여 과적합을 방지할 수 있는 수준의 initialize point를 잡았고, 이를 다시 supervised backpropagation를 사용하는 형태로 학습을 진행한다.
또한 2013년에는 신호처리학회인 ICASSP에서 RBM을 대체하여 과적합을 방지할 수 있는 Drop-out이라는 개념이 소개되면서 사전훈련 보다 훨씬 더 간단하고 강력한 형태로 과적합을 방지할 수 있게 되었다.

딥러닝이 주목받게 된 이유

첫 번째는 앞서 딥 러닝의 역사에서 언급한 바 있는 기존 인공신경망 모델의 단점이 극복되었다는 점이다. 그러나 과적합 문제만 해결되었다고 해서 느린 학습시간이 줄어드는 것은 아니다.
두 번째 이유로, 여기에는 하드웨어의 발전이라는 또 다른 요인이 존재 한다. 특히 강력한 GPU는 딥러닝에서 복잡한 행렬 연산에 소요되는 시간을 크게 단축시켰다.
세 번째 이유로 빅 데이터를 들 수 있다.
대량으로 쏟아져 나오는 데이터들, 그리고 그것들을 수집하기 위한 노력 특히 SNS 사용자들에 의해 생산되는 다량의 자료와 태그정보들 모두가 종합되고 분석 되어 학습에 이용될 수 있다.
- 인공신경망의 학습에 사용되는 트레이닝벡터는 이름이 붙어 있는(labeled) 데이터여야 하는데(supervised learning의 경우) 대량의 트레이닝셋 모두에 label을 달아주는 일은 불가능한 일이다. 이런 이유로 초기 학습에 사용되는 일부 데이터에 대해서만 지도학습(supervised learning)을 수행하고 나머지 트레이닝셋에 대해서는 비지도학습(unsupervised learning)을 진행하며, 학습된 결과는 기존 학습의 결과와 앞서 분석된 메타태그 정보들을 종합하여 인식기가 완성된다.

딥러닝의 다양한 분야

딥러닝은 다양한 분야, 특히 자동 음성 인식(ASR, automatic speech recognition)과 컴퓨터비전 분야에서 최고수준의 성능을 보여주고 있으며, 이들은 보통 딥러닝의 새로운 응용들의 지속적인 성능 향상을 위해 만들어진 TIMIT(Texas Instruments와 MIT가 제작한 음성 Database), MNIST(이미지 클러스터링을 위한 hand-written 숫자 이미지 데이터베이스로 National Institute of Standards and Technology가 제작) 등의 데이터베이스를 사용했다.
최근에는 Convolution Neural Networks 기반의 딥러닝 알고리즘이 뛰어난 성능을 발휘하고 있으며, 컴퓨터비전과 음성인식등의 분야에서 특히 탁월한 성능을 보이고 있다.

딥러닝의 겨울

1957년 프랑크 로젠블라트(Frank Rosenblatt)가 퍼셉트론 이론을 발표한 이래 인공신경망 이론은 효과적인 학습 모델을 찾지 못한 채 여러 가지 문제에 직면하게 됩니다.
역전파(backpropagation)법에서 인공신경망의 레이어(layer)가 늘어날수록 오래 전 데이터에서 기울기가 사라지는 문제(vanishing gradient problem)와 학습 데이터를 과하게 학습하여 학습 데이터에 대해서는 오차가 감소하지만 실제 데이터에 대해서는 오히려 오차가 증가하는 과적합(overfitting) 문제, 마지막으로 문제의 규모가 커질 때마다 나타나는 높은 시간 복잡도와 컴퓨터 성능의 한계 등으로 인해 인공신경망 이론은 큰 진전을 보지 못하고 정체 상태를 맞이하게 됩니다.

딥러닝의 부활

2006년 토론토 대학의 제프리 힌튼(Geoffrey Hinton) 교수는 심층 신뢰 신경망(Deep Belief Network, DBN)이라는 딥러닝에 매우 효과적인 알고리즘에 관한 논문을 발표합니다. 
제프리 힌튼 교수는 이 논문을 실제 적용하여 2012년 세계 최대 이미지 인식 경연대회인 ILSVRC에서 나머지 팀들이 26% 대의 이미지 인식 오류율로 각축을 벌일 때 홀로 15% 대의 오류율을 기록함으로써 1위를 차지하게 됩니다.
인공지능 분야의 전문가들 대부분은 제프리 힌튼 교수의 이 논문이 딥러닝의 부활을 알리는 계기가 되었다고 말하고 있습니다.
또한, 기울기가 사라지는 문제(vanishing gradient problem)를 해결하기 위해 기존에 사용하던 시그모이드(sigmoid) 함수 대신에 ReLU(Rectified Linear Unit)라는 함수가 새롭게 고안되었으며, 드롭아웃 계층(dropout layer)을 사용하여 학습 중일 때 랜덤하게 뉴런을 비활성화 함으로써 학습이 학습 데이터에 치우치는 과적합(overfitting) 문제를 해결하였습니다.
이러한 혁신적인 알고리즘의 개발과 더불어 컴퓨터 하드웨어의 급속한 발달, GPU를 활용한 병렬처리 기술의 개발 등으로 딥러닝은 획기적으로 그 성능이 향상되어 새로운 도약의 시대를 맞이하게 됩니다.

딥러닝의 도약과 그 원동력

딥러닝은 세계경제포럼 선정 2017년도 10대 미래유망기술, IEEE 컴퓨터 협회 선정 2018년도 10대 기술 트렌드 등 미래를 선도할 혁신 기술의 하나로 각광받고 있습니다.

딥러닝이 이렇게 빠르게 발전할 수 있었던 원동력은 다음과 같습니다.

1. GUP 기반의 병렬처리를 포함한 컴퓨팅 파워(computing power)의 발달
2. 인터넷을 통해 축적된 엄청난 양의 빅데이터(big data)
3. 딥러닝을 위한 획기적인 알고리즘의 고안

이 중에서도 빠른 시간 안에 더욱 빠르게 발전할 수 있는 가능성을 가지고 있는 분야는 바로 알고리즘 분야입니다.