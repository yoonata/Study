심층 신경망

다양한 종류의 심층 신경망 구조가 존재하지만, 대부분의 경우 대표적인 몇 가지 구조들에서 파생된 것이다. 
그렇지만 여러 종류의 구조들의 성능을 동시에 비교하는 것이 항상 가능한 것은 아닌데, 그 이유는 특정 구조들의 경우 주어진 데이터 집합에 적합하도록 구현되지 않은 경우도 있기 때문이다.

심층 신경망(Deep Neural Network, DNN)

심층 신경망(Deep Neural Network, DNN)은 입력층(input layer)과 출력층(output layer) 사이에 여러 개의 은닉층(hidden layer)들로 이뤄진 인공신경망(Artificial Neural Network, ANN)이다. 
심층 신경망은 일반적인 인공신경망과 마찬가지로 복잡한 비선형 관계(non-linear relationship)들을 모델링할 수 있다.
예를 들어, 사물 식별 모델을 위한 심층 신경망 구조에서는 각 객체가 이미지 기본 요소들의 계층적 구성으로 표현될 수 있다. 
이때, 추가 계층들은 점진적으로 모여진 하위 계층들의 특징들을 규합시킬 수 있다. 
심층 신경망의 이러한 특징은, 비슷하게 수행된 인공신경망에 비해 더 적은 수의 유닛(unit, node)들 만으로도 복잡한 데이터를 모델링할 수 있게 해준다.
이전의 심층 신경망들은 보통 앞먹임 신경망으로 설계되어 왔지만, 최근의 연구들은 심층 학습 구조들을 순환 신경망(Recurrent Neural Network, RNN)에 성공적으로 적용했다. 
일례로 언어 모델링(language modeling) 분야에 심층 신경망 구조를 적용한 사례 등이 있다.
합성곱 신경망(Convolutional Neural Network, CNN)의 경우에는 컴퓨터 비전(computer vision) 분야에서 잘 적용되었을 뿐만 아니라, 각각의 성공적인 적용 사례에 대한 문서화 또한 잘 되어 있다. 
더욱 최근에는 합성곱 신경망이 자동 음성인식 서비스(Automatic Response Service, ARS)를 위한 음향 모델링(acoustic modeling) 분야에 적용되었으며, 기존의 모델들 보다 더욱 성공적으로 적용되었다는 평가를 받고 있다.
심층 신경망은 표준 오류역전파 알고리즘으로 학습될 수 있다. 
이때, 가중치(weight)들은 아래의 등식을 이용한 확률적 경사 하강법(stochastic gradient descent)을 통하여 갱신될 수 있다.
△wij(t+1) = △wij(t) + η(∂C / ∂wij)
여기서, η는 학습률(learning rate)을 의미하며, C는 비용함수(cost function)를 의미한다. 
비용함수의 선택은 학습의 형태(지도 학습, 자율 학습 (기계 학습), 강화 학습 등)와 활성화함수(activation function)같은 요인들에 의해서 결정된다. 
예를 들면, 다중 클래스 분류 문제(multiclass classification problem)에 지도 학습을 수행할 때, 일반적으로 활성화함수와 비용함수는 각각 softmax 함수와 교차 엔트로피 함수(cross entropy function)로 결정된다.
softmax 함수는 Pj = exp(xj) / ∑k exp(xk) 로 정의된다. 
이때,  Pj 는 클래스 확률(class probability)을 나타내며,  Xj 와  Xk 는 각각 유닛 j로의 전체 입력(total input)과 유닛 k 로의 전체 입력을 나타낸다. 
교차 엔트로피는 C = -∑dj log(pj)로 정의된다. 
이때, dj 는 출력 유닛 j 에 대한 목표 확률(target probability)을 나타내며, Pj 는 해당 활성화함수를 적용한 이후의 j 에 대한 확률 출력(probability output)이다.
 
심층 신경망의 문제점

기존의 인공신경망과 같이, 심층 신경망 또한 나이브(naive)한 방식으로 학습될 경우 많은 문제들이 발생할 수 있다. 그 중 과적합과 높은 시간 복잡도가 흔히 발생하는 문제들이다.
심층 신경망이 과적합에 취약한 이유는 추가된 계층들이 학습 데이터의 rare dependency의 모형화가 가능하도록 해주기 때문이다. 
과적합을 극복하기 위해서 weight decay (l2–regularization) 또는 sparsity (l1–regularization) 와 같은 regularization 방법들이 사용될 수 있다. 
그리고 최근에 들어서는 심층 신경망에 적용되고 있는 정규화 방법 중 하나로 dropout 정규화가 등장했다. 
dropout 정규화에서는 학습 도중 은닉 계층들의 몇몇 유닛들이 임의로 생략된다. 
이러한 방법은 학습 데이터(training data)에서 발생할 수 있는 rare dependency를 해결하는데 도움을 준다.
오차역전파법과 경사 하강법은 구현의 용이함과 국지적 최적화(local optima)에 잘 도달한다는 특성으로 인해 다른 방법들에 비해 선호되어온 방법들이다. 
그러나 이 방법들은 심층 신경망을 학습 시킬 때 시간 복잡도가 매우 높다. 
심층 신경망을 학습시킬 때에는 크기(계층의 수 와 계층 당 유닛 수), 학습률, 초기 가중치 등 많은 매개변수(parameter)들이 고려되어야 한다. 
하지만 최적의 매개변수들을 찾기 위해 매개변수 공간 전부를 확인하는 것은 계산에 필요한 시간과 자원의 제약으로 인해 불가능하다. 
시간 복잡도를 해결하기 위해, 미니 배치(mini batch, 여러 학습 예제들의 경사를 동시에 계산), 드롭 아웃(drop out)과 같은 다양한 '묘책’들이 등장하였다. 
또한, 행렬 및 벡터 계산에 특화된 GPU는 많은 처리량을 바탕으로 두드러지는 학습 속도 향상을 보여주었다.

합성곱 신경망(Convolutional Neural Network, CNN)

합성곱 신경망(Convolutional Neural Network, CNN)은 최소한의 전처리(preprocess)를 사용하도록 설계된 다계층 퍼셉트론(multilayer perceptrons)의 한 종류이다. 
CNN은 하나 또는 여러개의 합성곱 계층과 그 위에 올려진 일반적인 인공 신경망 계층들로 이루어져 있으며, 가중치와 통합 계층(pooling layer)들을 추가로 활용한다. 
이러한 구조 덕분에 CNN은 2차원 구조의 입력 데이터를 충분히 활용할 수 있다. 
다른 딥 러닝 구조들과 비교해서, CNN은 영상, 음성 분야 모두에서 좋은 성능을 보여준다.
CNN은 또한 표준 역전달을 통해 훈련될 수 있다.
CNN은 다른 피드포워드 인공신경망 기법들보다 쉽게 훈련되는 편이고 적은 수의 매개변수를 사용한다는 이점이 있다.
최근 딥 러닝에서는 합성곱 심층 신뢰 신경망 (Convolutional Deep Belief Network, CDBN) 가 개발되었는데, 기존 CNN과 구조적으로 매우 비슷해서, 그림의 2차원 구조를 잘 이용할 수 있으며 그와 동시에 심층 신뢰 신경망 (Deep Belief Network, DBN)에서의 선훈련에 의한 장점도 취할 수 있다. 
CDBN은 다양한 영상과 신호 처리 기법에 사용될 수 있는 일반적인 구조를 제공하며 CIFAR 와 같은 표준 이미지 데이터에 대한 여러 벤치마크 결과에 사용되고 있다.

순환 신경망(Recurrent Neural Network, RNN)

순환 신경망은 인공신경망을 구성하는 유닛 사이의 연결이 Directed cycle을 구성하는 신경망을 말한다.
순환 신경망은 앞먹임 신경망과 달리, 임의의 입력을 처리하기 위해 신경망 내부의 메모리를 활용할 수 있다. 
이러한 특성에 의해 순환 신경망은 필기체 인식(Handwriting recognition)과 같은 분야에 활용되고 있고, 높은 인식률을 나타낸다.
순환 신경망을 구성할 수 있는 구조에는 여러가지 방식이 사용되고 있다.
완전 순환망(Fully Recurrent Network), Hopfield Network, Elman Network, Echo state network(ESN), Long short term memory network(LSTM), Bi-directional RNN, Continuous-time RNN(CTRNN), Hierarchical RNN, Second Order RNN 등이 대표적인 예이다.
순환 신경망을 훈련(Training)시키기 위해 대표적으로 경사 하강법, Hessian Free Optimization, Global Optimization Methods 방식이 쓰이고 있다. 
하지만 순환 신경망은 많은 수의 뉴런 유닛이나 많은 수의 입력 유닛이 있는 경우에 훈련이 쉽지 않은 스케일링 이슈를 가지고있다.

제한 볼츠만 머신 (Restricted Boltzmann Machine, RBM)

볼츠만 머신에서, 층간 연결을 없앤 형태의 모델이다. 
층간 연결을 없애면, 머신은 가시 유닛(Visible Unit)과 은닉 유닛(Hidden Unit)으로 이루어진 무방향 이분 그래프 형태의 모양이 된다. 
결론적으로 모델의 층간 연결을 없앰으로써, 얻는 이점으로 뉴럴 네트워크는 깊어질 수 있었다. 
가장 큰 이점은 가시 유닛이 관찰되고 고정(Clamped)되었을 때 은닉 유닛을 추론(Inference) 하는 MCMC 과정이 단 한 번에 끝난다는 것이다.
RBM은 확률모델의 계산이 불가능하기 때문에, 학습시 근사법인 MCMC 나 또는 제프리 힌튼 교수가 발견한 CD(Contrastive Divergence) 를 사용하는데, RBM의 모델의 간단함에서 오는 추론에 대한 이점은 샘플링 근간의 학습법이 실용적인 문제에 적용되는데 기여했다. 
자세한 학습 방법은 아래에 설명 되어 있다.
RBM은 DBN의 기본 뼈대가 된다, RBM을 쌓아올리면서(Stacking), Greedy하게 학습함으로써, DBN을 완성한다. 
DBN을 아는 사람은 기본적으로 RBM은 무방향이기에, RBM이 방향성 있는 모델인 DBN이 되는지 의아할 것이다. 
이는 제프리 힌튼 의 제자인 Teh, Y. W 의 우연 발견적인 연구 성과인데, 원래 힌튼 교수는 Graphical Model 이 방향성을 가질 때 생기는 Explaining Away 효과 때문에 학습이 매우 어려워 연구를 무방향성 모델로 선회했다가, RBM을 쌓으면, 다층 RBM이 되는 것이 아니라, DBN과 비슷해진다는 사실을 발견하게 된다. 
이 발견은 DBN학습을 단순히 여러 개의 RBM 학습으로 환원시킴으로써(앞서서 얘기했듯이 RBM은 학습이 어렵지 않다) 어려운 DBN 학습의 복잡도를 층의 갯수에 비례하는 복잡도로 낮추었다. 이는 선행학습(Pre-training) 과 미세조정(fine-tuning) 의 새로운 학습 패러다임으로 발전하게 된다.
RBM 훈련 과정에서의 가중치 갱신은, 다음의 식을 기반으로 경사 하강법을 통해 이루어진다.
△wij(t+1) = △wij(t) + η(∂log(p(v)) / ∂wij)
여기서 p(v)는 다음의 식으로 주어지는 가시 벡터의 확률을 나타내고,
p(v) = (1/Z)∑e^(-E(v,h)) Z 는 정규화를 위해 사용되는 분배함수(partition function)이며, E(v, h) 는 신경망의 상태(state)에 부여되는 에너지 함수이다. 
에너지가 낮을수록 해당 신경망이 더 적합한 상태임을 의미한다. 기울기 ∂log(p(v)) / ∂wij는 좀 더 간단하게 <vi hj>data - <vi hj>model로 표현이 되는데 이 때 <...>p 는 분포 p에 관한 평균을 의미한다.  
<vi hj>model 을 샘플링 하려면 alternating Gibbs sampling을 매우 많이 반복해야 하기 때문에 CD에서는 alternating Gibbs sampling을 n 회만 반복한다.
실험을 통해 n = 1 일 경우에도 충분한 성능이 나온다고 알려져 있다.
n회 반복 후 샘플링 된 데이터가 <vi hj>model를 대신한다. 
CD 수행 과정은 다음과 같다:

1. 가시 유닛들을 훈련 벡터로 초기화 한다.
2. 다음의 가시 유닛들이 주어지면 은닉 유닛들을 모두 동시에 갱신한다:
p(hj = 1 | V) = σ(bj + ∑vi wij) . σ는 시그모이드 함수(sigmoid function)를 의미하며, bj는 hj의 편향이다.
3. 다음의 은닉 유닛들이 주어지면 가시 유닛들을 모두 동시에 갱신한다:
p(vi = 1 | H) = σ(ai + ∑hj wij) . ai는  vi의 편향이다. 이 부분을 "복원(reconstruction)"이라고 부른다.
4. 복원된 가시 유닛들이 주어지면 2 에서 쓰인 식을 이용하여 은닉 유닛들을 모두 동시에 다시 갱신한다.
5. 다음과 같이 가중치를 갱신한다.
△wij ∝ <vi hj>data - <vi hj>reconstruction

심층 신뢰 신경망(Deep Belief Network, DBN)

심층 신뢰 신경망(Deep Belief Network, DBN)이란 기계학습에서 사용되는 그래프 생성 모형(generative graphical model)으로, 딥 러닝에서는 잠재변수(latent variable)의 다중계층으로 이루어진 심층 신경망을 의미한다. 
계층 간에는 연결이 있지만 계층 내의 유닛 간에는 연결이 없다는 특징이 있다.
DBN은 생성 모형이라는 특성상 선행학습에 사용될 수 있고, 선행학습을 통해 초기 가중치를 학습한 후 역전파 혹은 다른 판별 알고리즘을 통해 가중치의 미조정을 할 수 있다. 
이러한 특성은 훈련용 데이터가 적을 때 굉장히 유용한데, 이는 훈련용 데이터가 적을수록 가중치의 초기값이 결과적인 모델에 끼치는 영향이 세지기 때문이다.
선행학습된 가중치 초기값은 임의로 설정된 가중치 초기값에 비해 최적의 가중치에 가깝게 되고 이는 미조정 단계의 성능과 속도향상을 가능케 한다.
DBN은 비지도 방식으로 계층마다 학습을 진행하는데 이때 각각의 계층은 보통 RBM의 형태를 띄고 있다. 
RBM들을 쌓아 올리면서 DBN을 훈련시키는 방법에 대한 설명은 아래에 제공 되어 있다. 
RBM은 에너지 기반의 생성 모형으로 가시 유닛과 은닉 유닛으로 이루어진 무방향 이분 그래프 형태이다. 
가시 유닛들과 은닉 유닛들 사이에만 연결이 존재한다.
RBM이 훈련되고 나면 다른 RBM이 그 위에 쌓아 올려짐으로써 다중 계층 모형을 형성한다. 
RBM이 쌓아 올려질 때마다, 이미 훈련된 RBM의 최상위 계층이 새로 쌓이는 RBM의 입력으로 쓰인다. 
이 입력을 이용하여 새 RBM이 훈련되고, 원하는 만큼의 계층이 쌓일 때까지 해당 과정이 반복된다.
실험결과에 따르면, CD의 최대가능도 근사가 굉장히 투박함에도 불구하고, 심층 신경망 구조를 학습하기에는 충분히 효과적인 방식이라고 한다.

심층 Q-네트워크(Deep Q-Networks)

- 강화 학습을 위한 가장 최신 딥 러닝 모델

강화 학습(Reinforcement learning)은 기계 학습의 한 영역이다.
행동심리학에서 영감을 받았으며, 어떤 환경 안에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법이다. 
이러한 문제는 매우 포괄적이기 때문에 게임 이론, 제어이론, 운용 과학, 정보 이론, 시뮬레이션 기반 최적화, 다중 에이전트 시스템, 떼 지능, 통계학, 유전 알고리즘 등의 분야에서도 연구된다. 
운용 과학과 제어 이론에서 강화 학습이 연구되는 분야는 "근사 동적 계획법"이라고 불린다. 
또한 최적화 제어 이론에서도 유사한 문제를 연구하지만, 대부분의 연구가 최적해의 존재와 특성에 초점을 맞춘다는 점에서 학습과 근사의 측면에서 접근하는 강화 학습과는 다르다. 
경제학과 게임 이론 분야에서 강화 학습은 어떻게 제한된 합리성 하에서 평형이 일어날 수 있는지를 설명하는 데에 사용되기도 한다.
강화 학습에서 다루는 '환경'은 주로 마르코프 결정 과정으로 주어진다. 
마르코프 결정 과정 문제를 해결하는 기존의 방식과 강화 학습이 다른 지점은, 강화 학습은 마르코프 결정 과정에 대한 지식을 요구하지 않는다는 점과, 강화 학습은 크기가 매우 커서 결정론적 방법을 적용할 수 없는 규모의 마르코프 결정 과정 문제를 다룬다는 점이다.
강화 학습은 또한 입출력 쌍으로 이루어진 훈련 집합이 제시되지 않으며, 잘못된 행동에 대해서도 명시적으로 정정이 일어나지 않는다는 점에서 일반적인 지도 학습과 다르다. 
대신, 강화학습의 초점은 학습 과정에서의(on-line) 성능이며, 이는 탐색(exploration)과 이용(exploitation)의 균형을 맞춤으로써 제고된다.
탐색과 이용의 균형 문제 강화 학습에서 가장 많이 연구된 문제로, 다중 슬롯 머신 문제(multi-armed bandit problem)와 유한한 마르코프 결정 과정 등에서 연구되었다.

딥러닝 알고리즘

딥러닝에 사용되는 인공신경망 알고리즘에는 심층 신경망(DNN), 컨볼루션 신경망(CNN), 순환 신경망(RNN), 제한 볼츠만 머신(RBM), 심층 신뢰 신경망(DBN), 심층 Q-네트워크(Deep Q-Networks) 등 다양한 형태의 수많은 알고리즘이 각각의 장단점을 가지고 활용되고 있습니다.
특정 형태의 문제를 잘 해결하는 알고리즘이 다른 형태의 문제는 잘 풀지 못 할 수도 있으며, 이것이 이렇게 다양한 알고리즘이 개발되어 사용되고 있는 이유 중 하나입니다. 
상황에 알맞게 제대로 고안된 딥러닝 알고리즘은 인간의 사고와 분석의 한계를 넘어 엄청난 양의 데이터로부터 비즈니스적 가치(value)를 창출할 수 있도록 도와줍니다.

인공신경망 핵심 알고리즘

딥러닝에 사용되는 인공신경망 알고리즘은 매우 다양하지만 몇 가지의 대표적인 알고리즘에서 파생된 것들이 대부분입니다.

다음은 대표적인 인공신경망 알고리즘 중에서도 가장 기본적이고 이해하기 쉬운 알고리즘입니다.

1. 컨볼루션 신경망(Convolutional Neural Network, CNN)
2. 심층 신뢰 신경망(Deep Belief Network, DBN)

컨볼루션 신경망(Convolutional Neural Network, CNN)

비전(vision) 분야에서 다층 퍼셉트론(MultiLayer Perceptron, MLP)을 이용하면 이론적으로 학습은 가능하지만 영상의 크기가 커질수록 학습해야 하는 데이터의 크기나 학습 시간이 매우 커지게 되며, 이미지의 위치, 각도, 크기 변화에도 취약해지는 단점을 가지게 됩니다.
이러한 문제점을 해결하기 위해 고안된 지도 학습 알고리즘인 컨볼루션 신경망(CNN)은 인간의 시신경 구조를 모방하여 만들어졌으며, 특징(feature)을 추출하는 일종의 필터인 컨볼루션 커널(convolution kernel)을 도입하여 입력된 이미지를 분류하기 위한 변별적 학습을 수행합니다.
또한, 최대 풀링(max pooling)과 평균 풀링(average pooling)과 같은 서브 샘플링(sub-sampling)을 통해 이웃하고 있는 데이터 간의 대비율(contrast)을 높이고 처리해야 할 데이터의 양을 줄여줍니다.
컨볼루션 커널을 이용한 필터링 단계와 풀링을 이용한 서브 샘플링 단계를 여러 번 반복함으로써 CNN은 이미지의 위치나 각도 변화 등에도 변함없이 강건함(topology invariance)을 유지할 수 있게 됩니다.
CNN은 이미지의 추상적인 특징을 여러 관점에서 추출함으로써 위치에 무관한 특징을 추출하고, 학습해야 할 전체 매개변수의 수를 감소시켜 빠른 학습 속도와 우수한 일반화 능력을 가질 수 있도록 도와줍니다.

심층 신뢰 신경망(Deep Belief Network, DBN)

심층 신뢰 신경망(DBN)은 입력층과 하나의 은닉층으로 구성되어 있는 제한 볼츠만 머신(Restricted Boltzmann Machine, RBM)을 빌딩블럭(building block)과 같이 여러 층으로 쌓아 올린 형태의 신경망으로, 입력 데이터와 같은 출력을 재생성하는 모델입니다.
여러 층으로 이루어진 인공신경망 학습에서는 신경망의 레이어(layer)가 늘어날수록 오래 전 데이터에서 기울기가 사라지는 문제(vanishing gradient problem)가 발생합니다. 
이 문제를 해결하기 위해 DBN에서는 입력으로부터 가까운 층부터 차례대로 RBM을 이용한 일종의 비지도 학습인 사전 학습을 수행하며, 이것을 층별 선훈련(layer-wise pre-training)이라고 합니다.
DBN에서 1단계 RBM의 사전 학습이 완료되면 그 결괏값이 2단계 RBM의 입력값으로 사용되며, 이때 1단계에서 사용된 가중치는 고정됩니다. 이와 같은 방법으로 마지막 은닉층까지 순서대로 반복되며, 이와 같은 층별 선훈련의 목적은 각 은닉층에 존재하는 가중치를 가능한 한 목푯값에 가깝도록 만드는 것입니다.
DBN은 비지도 학습인 RBM을 기반으로 여러 번의 사전 학습을 통해 가중치를 어느 정도 보정하고, 역전파 및 피드포워드 알고리즘을 통해 최종 가중치를 계산하게 됩니다. 이러한 특성은 학습 데이터의 양이 적을 때 굉장히 유용하게 사용됩니다.
이러한 DBN은 입력 데이터와 같은 출력 데이터를 재생성함으로 오토인코더나 분류기 등에 활용될 수 있으며, 이미지에서 사람의 얼굴 방향 인식 문제나 문서의 코드화 작업 등에 사용되고 있습니다.

 

